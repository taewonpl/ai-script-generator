# âš ï¸ 503 ì˜¤ë¥˜ ê¸‰ì¦ ì²˜ë¦¬ Runbook

## ğŸ“‹ ê°œìš”

HTTP 503 Service Unavailable ì˜¤ë¥˜ ê¸‰ì¦ ì‹œ ì‹ ì†í•œ ì§„ë‹¨ ë° ëŒ€ì‘ì„ ìœ„í•œ ìš´ì˜ ì ˆì°¨ì…ë‹ˆë‹¤.

## ğŸš¨ ì¦ìƒ ì‹ë³„

### ì£¼ìš” ì¦ìƒ
- `http_requests_total{status="503"}` ë©”íŠ¸ë¦­ ê¸‰ì¦
- ì‚¬ìš©ìë¡œë¶€í„° "ì„œë¹„ìŠ¤ ì´ìš© ë¶ˆê°€" ì‹ ê³  ì¦ê°€
- í”„ë¡ íŠ¸ì—”ë“œì—ì„œ API í˜¸ì¶œ ì‹¤íŒ¨ìœ¨ ìƒìŠ¹
- ë¡œë“œë°¸ëŸ°ì„œ í—¬ìŠ¤ì²´í¬ ì‹¤íŒ¨

### ì•Œë¦¼ ì„ê³„ê°’
```yaml
# Prometheus ì•Œë¦¼ ì¡°ê±´
- alert: HTTP503ErrorSpike
  expr: rate(http_requests_total{status="503"}[5m]) > 20
  for: 2m
  labels:
    severity: critical
```

### ë¡œê·¸ì—ì„œ í™•ì¸í•  íŒ¨í„´
```
ERROR: Service unavailable - upstream timeout
WARNING: All backend servers are down
ERROR: Connection pool exhausted
INFO: Scaling up workers due to high load
```

---

## ğŸ” ì§„ë‹¨ ë‹¨ê³„

### 1ë‹¨ê³„: ì„œë¹„ìŠ¤ ìƒíƒœ ì¦‰ì‹œ í™•ì¸
```bash
# ì „ì²´ ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬
curl -f http://localhost:8001/health  # Project Service
curl -f http://localhost:8002/health  # Generation Service
curl -f http://localhost:3000         # Frontend

# ì‘ë‹µ ì‹œê°„ ì¸¡ì •
time curl -s http://localhost:8002/api/generation/health
```

### 2ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  í™•ì¸
```bash
# CPU ì‚¬ìš©ë¥  (80% ì´ìƒì´ë©´ ìœ„í—˜)
top -b -n1 | grep "Cpu(s)"

# ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  (85% ì´ìƒì´ë©´ ìœ„í—˜)
free -m | awk 'NR==2{printf "Memory Usage: %s/%sMB (%.2f%%)\n", $3,$2,$3*100/$2}'

# ë””ìŠ¤í¬ I/O í™•ì¸
iostat -x 1 3

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìˆ˜
ss -tuln | wc -l
```

### 3ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸
```bash
# SQLite ì ê¸ˆ ìƒíƒœ í™•ì¸
lsof | grep "\.db$" | wc -l

# Redis ì—°ê²° ìƒíƒœ í™•ì¸
redis-cli ping
redis-cli info clients

# ChromaDB ìƒíƒœ í™•ì¸
curl -f http://localhost:8000/api/v1/heartbeat
```

### 4ë‹¨ê³„: ì—…ìŠ¤íŠ¸ë¦¼ ì„œë¹„ìŠ¤ í™•ì¸
```bash
# OpenAI API ìƒíƒœ í™•ì¸
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models | jq '.data | length'

# ì™¸ë¶€ ì„œë¹„ìŠ¤ ì‘ë‹µ ì‹œê°„ ì¸¡ì •
time curl -s https://api.openai.com/v1/models > /dev/null
```

---

## âš¡ ì¦‰ì‹œ ëŒ€ì‘ ì¡°ì¹˜ (ìš°ì„ ìˆœìœ„ë³„)

### ìš°ì„ ìˆœìœ„ 1: íŠ¸ë˜í”½ ì œì–´ (30ì´ˆ ì´ë‚´)
```bash
# 1. Rate limiting ê°•í™”
curl -X POST http://localhost:8002/api/admin/rate-limit/emergency \
     -d '{"requests_per_minute": 100, "burst": 10}' \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer ${ADMIN_TOKEN}"

# 2. ë¹„í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸ ì¼ì‹œ ì°¨ë‹¨
curl -X POST http://localhost:8002/api/admin/circuit-breaker/enable \
     -d '{"endpoints": ["/api/generation/batch", "/api/rag/search"]}'

# 3. ë¡œë“œë°¸ëŸ°ì„œì—ì„œ íŠ¸ë˜í”½ ì‰ì´í•‘
# Nginx upstream ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸
nginx -s reload
```

### ìš°ì„ ìˆœìœ„ 2: ë¦¬ì†ŒìŠ¤ í™•ë³´ (2ë¶„ ì´ë‚´)
```bash
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì€ í”„ë¡œì„¸ìŠ¤ ì¬ì‹œì‘
if [ $(free | awk '/^Mem:/{print $3/$2 * 100.0}' | cut -d. -f1) -gt 85 ]; then
    echo "High memory usage detected, restarting services"
    
    # Generation service ì¬ì‹œì‘ (ë¬´ì¤‘ë‹¨)
    curl -X POST http://localhost:8002/api/admin/graceful-restart
    
    # Worker processes ìŠ¤ì¼€ì¼ ë‹¤ìš´
    curl -X POST http://localhost:8002/api/admin/workers/scale \
         -d '{"target_workers": 2}'  # ê¸°ë³¸ 4ê°œì—ì„œ 2ê°œë¡œ ì¶•ì†Œ
fi

# 2. ì„ì‹œ íŒŒì¼ ì •ë¦¬
find /tmp -name "ai-script-*" -mmin +60 -delete
docker system prune -f  # Docker ì‚¬ìš© ì‹œ

# 3. ë¡œê·¸ íŒŒì¼ ì •ë¦¬ (ë””ìŠ¤í¬ ê³µê°„ í™•ë³´)
find /var/log -name "*.log" -mtime +7 -exec gzip {} \;
```

### ìš°ì„ ìˆœìœ„ 3: ì„œë¹„ìŠ¤ ë³µêµ¬ (5ë¶„ ì´ë‚´)
```bash
# 1. ì„œë¹„ìŠ¤ë³„ ìˆœì°¨ ì¬ì‹œì‘
systemctl restart ai-script-project-service
sleep 30
systemctl restart ai-script-generation-service
sleep 30

# 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì´ˆê¸°í™”
curl -X POST http://localhost:8001/api/admin/db/reset-pool
curl -X POST http://localhost:8002/api/admin/db/reset-pool

# 3. Redis ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ í™•ë³´)
redis-cli FLUSHDB 1  # ì„ì‹œ ë°ì´í„°ë§Œ ì‚­ì œ (DB 1)

# 4. í—¬ìŠ¤ì²´í¬ ê°•ì œ ì„±ê³µ (ì¼ì‹œì )
curl -X POST http://localhost:8002/api/admin/health/force-ok \
     -d '{"duration_minutes": 10}'
```

---

## ğŸ› ï¸ ê·¼ë³¸ ì›ì¸ë³„ ëŒ€ì‘

### ì‹œë‚˜ë¦¬ì˜¤ 1: ë¦¬ì†ŒìŠ¤ ê³ ê°ˆ (CPU/Memory)
```bash
# ì§„ë‹¨
htop -p $(pgrep -f "python.*generation-service")

# ëŒ€ì‘
if [ CPU_USAGE -gt 80 ]; then
    # ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ìˆ˜ ì¶•ì†Œ
    export RAG_MAX_CONCURRENT_JOBS=10  # ê¸°ë³¸ 50ì—ì„œ ì¶•ì†Œ
    export RAG_EMBEDDING_CONCURRENCY=1  # ê¸°ë³¸ 3ì—ì„œ ì¶•ì†Œ
    
    # ì„œë¹„ìŠ¤ ì¬ì‹œì‘ìœ¼ë¡œ ì„¤ì • ì ìš©
    systemctl restart ai-script-generation-service
fi

if [ MEMORY_USAGE -gt 85 ]; then
    # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì¤‘ë‹¨
    curl -X POST http://localhost:8002/api/admin/jobs/pause-heavy \
         -d '{"job_types": ["batch_generation", "large_document_rag"]}'
    
    # ìºì‹œ í¬ê¸° ì¶•ì†Œ
    redis-cli CONFIG SET maxmemory 512mb
fi
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ë°ì´í„°ë² ì´ìŠ¤ ë³‘ëª©
```bash
# SQLite ì ê¸ˆ ì§„ë‹¨
lsof /path/to/database.db

# ëŒ€ì‘: ì½ê¸° ì „ìš© ë³µì œë³¸ìœ¼ë¡œ íŠ¸ë˜í”½ ë¶„ì‚°
export DATABASE_READ_REPLICA_URL="sqlite:///readonly.db"

# ì¥ê¸°ê°„ ì‹¤í–‰ ì¤‘ì¸ ì¿¼ë¦¬ ê°•ì œ ì¢…ë£Œ
curl -X POST http://localhost:8001/api/admin/db/kill-long-queries \
     -d '{"max_duration_seconds": 30}'
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: ì™¸ë¶€ API ì¥ì•  (OpenAI/Anthropic)
```bash
# API ìƒíƒœ í™•ì¸
curl -s https://status.openai.com/api/v2/status.json | jq '.status.description'

# ëŒ€ì‘: Circuit breaker í™œì„±í™”
curl -X POST http://localhost:8002/api/admin/circuit-breaker/openai/enable
curl -X POST http://localhost:8002/api/admin/circuit-breaker/anthropic/enable

# Fallback ëª¨ë¸ë¡œ ì „í™˜
export FALLBACK_MODEL_ENABLED=true
export PRIMARY_MODEL="gpt-3.5-turbo"  # ë” ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ì „í™˜
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: ë„¤íŠ¸ì›Œí¬ ì—°ê²° í¬í™”
```bash
# ì—°ê²° ìˆ˜ í™•ì¸
ss -s | grep TCP

# ëŒ€ì‘: ì—°ê²° í’€ í¬ê¸° ì¡°ì •
curl -X POST http://localhost:8002/api/admin/connection-pool/resize \
     -d '{"max_connections": 50, "min_connections": 5}'

# Keep-alive ì‹œê°„ ë‹¨ì¶•
echo 'net.ipv4.tcp_keepalive_time = 300' >> /etc/sysctl.conf
sysctl -p
```

---

## ğŸ“Š ìë™ ìŠ¤ì¼€ì¼ë§ í™œì„±í™”

### ìˆ˜í‰ì  ìŠ¤ì¼€ì¼ë§ (ì»¨í…Œì´ë„ˆ í™˜ê²½)
```bash
# Kubernetes HPA ì¦‰ì‹œ í™œì„±í™”
kubectl autoscale deployment generation-service \
  --cpu-percent=70 \
  --min=2 \
  --max=10

# Docker Swarm ìŠ¤ì¼€ì¼ë§
docker service scale ai-script-generation=5
```

### ìˆ˜ì§ì  ìŠ¤ì¼€ì¼ë§ (ë‹¨ì¼ ì„œë²„)
```bash
# í”„ë¡œì„¸ìŠ¤ ì›Œì»¤ ìˆ˜ ë™ì  ì¦ê°€
curl -X POST http://localhost:8002/api/admin/workers/auto-scale \
     -d '{
       "enable": true,
       "min_workers": 2,
       "max_workers": 8,
       "cpu_threshold": 70,
       "memory_threshold": 80
     }'
```

---

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§ ê°•í™”

### ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ í™•ì¸
```bash
# Grafana ëŒ€ì‹œë³´ë“œ URL
echo "Check: https://monitoring.ai-script-generator.com/d/503-errors"

# ì£¼ìš” ë©”íŠ¸ë¦­ CLI ì¡°íšŒ
curl -s "http://localhost:9090/api/v1/query?query=rate(http_requests_total{status=\"503\"}[5m])" \
  | jq '.data.result[0].value[1]'
```

### ì¶”ê°€ ì•Œë¦¼ ì„¤ì •
```yaml
# ê¸´ê¸‰ ì•Œë¦¼ í™œì„±í™”
- alert: CriticalServiceDown
  expr: up{job="generation-service"} == 0
  for: 1m
  labels:
    severity: critical
    pager: "true"
  annotations:
    summary: "Generation service is completely down"
```

---

## âœ… ë³µêµ¬ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì¦‰ì‹œ í™•ì¸ (ë³µêµ¬ í›„ 5ë¶„ ì´ë‚´)
- [ ] HTTP 503 ì˜¤ë¥˜ìœ¨ < 1%
- [ ] í‰ê·  ì‘ë‹µ ì‹œê°„ < 2ì´ˆ
- [ ] í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ ì •ìƒ ì‘ë‹µ
- [ ] í•µì‹¬ ê¸°ëŠ¥ ìˆ˜ë™ í…ŒìŠ¤íŠ¸ í†µê³¼

### ì•ˆì •ì„± í™•ì¸ (ë³µêµ¬ í›„ 30ë¶„ ì´ë‚´)
- [ ] CPU ì‚¬ìš©ë¥  < 70%
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  < 80%
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìˆ˜ ì •ìƒ
- [ ] ì—ëŸ¬ ë¡œê·¸ ì¦ê°€ ì¤‘ë‹¨

### ì„±ëŠ¥ í™•ì¸ (ë³µêµ¬ í›„ 1ì‹œê°„ ì´ë‚´)
```bash
# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
ab -n 1000 -c 10 http://localhost:8002/api/generation/health

# ê²°ê³¼ í™•ì¸: 99%ê°€ 2ì´ˆ ì´ë‚´ ì‘ë‹µí•´ì•¼ í•¨
# Requests per second: > 50
# Time per request (mean): < 2000ms
```

---

## ğŸ”„ ì •ìƒí™” í›„ ì¡°ì¹˜

### Rate Limiting ì •ìƒí™”
```bash
# ê¸´ê¸‰ ì œí•œ í•´ì œ
curl -X DELETE http://localhost:8002/api/admin/rate-limit/emergency

# ì •ìƒ ì„¤ì • ë³µì›
curl -X POST http://localhost:8002/api/admin/rate-limit/restore-default
```

### Circuit Breaker í•´ì œ
```bash
# ë¹„í•„ìˆ˜ ì—”ë“œí¬ì¸íŠ¸ ì¬ê°œ
curl -X POST http://localhost:8002/api/admin/circuit-breaker/disable-all

# ì™¸ë¶€ API Circuit Breaker í•´ì œ (ë‹¨ê³„ì )
curl -X POST http://localhost:8002/api/admin/circuit-breaker/openai/test
sleep 300  # 5ë¶„ ëŒ€ê¸° í›„
curl -X POST http://localhost:8002/api/admin/circuit-breaker/openai/disable
```

### ë¦¬ì†ŒìŠ¤ ì„¤ì • ë³µì›
```bash
# ì›Œì»¤ ìˆ˜ ì •ìƒí™”
export RAG_MAX_CONCURRENT_JOBS=50
export RAG_EMBEDDING_CONCURRENCY=3
systemctl restart ai-script-generation-service
```

---

## ğŸ“ ì‚¬í›„ ë¶„ì„ í•„ìˆ˜ í•­ëª©

### 1. íƒ€ì„ë¼ì¸ ì‘ì„±
- ì¥ì•  ë°œìƒ ì‹œê°
- ì•Œë¦¼ ìˆ˜ì‹  ì‹œê°
- ëŒ€ì‘ ì‹œì‘ ì‹œê°
- ê° ì¡°ì¹˜ë³„ ì‹¤í–‰ ì‹œê°
- ì™„ì „ ë³µêµ¬ ì‹œê°

### 2. ì˜í–¥ ë²”ìœ„ ë¶„ì„
```sql
-- ì¥ì•  ê¸°ê°„ ì¤‘ ì‹¤íŒ¨í•œ ìš”ì²­ ìˆ˜
SELECT COUNT(*) FROM api_logs 
WHERE status_code = 503 
AND timestamp BETWEEN '${START_TIME}' AND '${END_TIME}';

-- ì˜í–¥ë°›ì€ ì‚¬ìš©ì ìˆ˜
SELECT COUNT(DISTINCT user_id) FROM api_logs 
WHERE status_code >= 500 
AND timestamp BETWEEN '${START_TIME}' AND '${END_TIME}';
```

### 3. ê·¼ë³¸ ì›ì¸ ë¶„ì„ (5 Whys)
1. Why did 503 errors spike?
2. Why did the servers become overloaded?
3. Why wasn't auto-scaling triggered earlier?
4. Why didn't monitoring catch this sooner?
5. Why don't we have better capacity planning?

### 4. ì¬ë°œ ë°©ì§€ ì¡°ì¹˜
- [ ] ì•Œë¦¼ ì„ê³„ê°’ ì¡°ì •
- [ ] ìë™ ìŠ¤ì¼€ì¼ë§ ì •ì±… ê°œì„ 
- [ ] ìš©ëŸ‰ ê³„íš ìˆ˜ë¦½
- [ ] ëª¨ë‹ˆí„°ë§ ê°•í™”
- [ ] ì¥ì•  ëŒ€ì‘ ì ˆì°¨ ê°œì„ 

---

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](../docs/architecture/overview.md)
- [ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ](https://monitoring.ai-script-generator.com)
- [ìë™ ìŠ¤ì¼€ì¼ë§ ê°€ì´ë“œ](../docs/operations/auto-scaling.md)
- [ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ](../docs/operations/performance-tuning.md)
- [ì¥ì•  ëŒ€ì‘ ì²´í¬ë¦¬ìŠ¤íŠ¸](../docs/operations/incident-response.md)