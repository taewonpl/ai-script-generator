# Prometheus Alerting Rules
# AI Script Generator v3 í”„ë¡œë•ì…˜ ì•Œë¦¼ ì„¤ì •

groups:
  - name: ai-script-generator-critical
    interval: 30s
    rules:
      # DLQ ì¦ê°€ ì•Œë¦¼
      - alert: DLQEntriesIncreasing
        expr: increase(dlq_entries_total[10m]) > 5
        for: 2m
        labels:
          severity: warning
          service: rag-worker
        annotations:
          summary: "DLQ entries increasing rapidly"
          description: "Dead Letter Queue has {{ $value }} new entries in the last 10 minutes. Investigate worker failures."
          runbook: "https://docs.ai-script-generator.com/runbooks/dlq-troubleshooting"

      # 10ë¶„ ì‹¤íŒ¨ìœ¨ > 3% ì•Œë¦¼
      - alert: HighRAGFailureRate
        expr: |
          (
            rate(rag_job_failures_total[10m]) / 
            (rate(rag_job_successes_total[10m]) + rate(rag_job_failures_total[10m]))
          ) * 100 > 3
        for: 5m
        labels:
          severity: critical
          service: rag-worker
        annotations:
          summary: "High RAG job failure rate"
          description: "RAG job failure rate is {{ $value }}% over the last 10 minutes, exceeding 3% threshold."
          runbook: "https://docs.ai-script-generator.com/runbooks/high-failure-rate"

      # 429 ê¸‰ì¦ ì•Œë¦¼
      - alert: RateLimitingSpike
        expr: increase(http_requests_total{status="429"}[5m]) > 20
        for: 1m
        labels:
          severity: warning
          service: api-gateway
        annotations:
          summary: "Rate limiting spike detected"
          description: "{{ $value }} HTTP 429 responses in the last 5 minutes. Users are being rate limited."
          runbook: "https://docs.ai-script-generator.com/runbooks/rate-limiting"

      # Memory token ì‚¬ìš©ë¥  > 35% (5ë¶„)
      - alert: HighMemoryTokenUsage
        expr: memory_token_used_pct > 35
        for: 5m
        labels:
          severity: warning
          service: memory-system
        annotations:
          summary: "High memory token usage"
          description: "Memory token usage is {{ $value }}% for 5 minutes, exceeding 35% threshold."
          runbook: "https://docs.ai-script-generator.com/runbooks/memory-management"

  - name: ai-script-generator-performance
    interval: 60s
    rules:
      # RAG ìž‘ì—… ëŒ€ê¸°ì—´ ê¸¸ì´ ì•Œë¦¼
      - alert: RAGQueueLengthHigh
        expr: rag_queue_length > 50
        for: 5m
        labels:
          severity: warning
          service: rag-worker
        annotations:
          summary: "RAG queue length is high"
          description: "RAG processing queue has {{ $value }} pending jobs for 5 minutes."
          runbook: "https://docs.ai-script-generator.com/runbooks/queue-scaling"

      # P95 ìž‘ì—… ì²˜ë¦¬ ì‹œê°„ > 60ì´ˆ
      - alert: SlowRAGProcessing
        expr: histogram_quantile(0.95, rag_job_duration_ms_bucket) > 60000
        for: 10m
        labels:
          severity: warning
          service: rag-worker
        annotations:
          summary: "RAG processing is slow"
          description: "95th percentile RAG job duration is {{ $value }}ms, exceeding 60s threshold."
          runbook: "https://docs.ai-script-generator.com/runbooks/performance-tuning"

      # SSE ì—°ê²° ëŠê¹€ ê¸‰ì¦
      - alert: SSEReconnectionSpike
        expr: increase(sse_reconnect_count[10m]) > 100
        for: 2m
        labels:
          severity: warning
          service: sse-gateway
        annotations:
          summary: "High SSE reconnection rate"
          description: "{{ $value }} SSE reconnections in the last 10 minutes."
          runbook: "https://docs.ai-script-generator.com/runbooks/sse-troubleshooting"

  - name: ai-script-generator-system
    interval: 60s
    rules:
      # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì•Œë¦¼
      - alert: HighCPUUsage
        expr: system_cpu_usage_pct > 80
        for: 10m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High CPU usage"
          description: "System CPU usage is {{ $value }}% for 10 minutes."

      - alert: HighMemoryUsage
        expr: system_memory_usage_pct > 85
        for: 10m
        labels:
          severity: critical
          service: system
        annotations:
          summary: "High memory usage"
          description: "System memory usage is {{ $value }}% for 10 minutes."

      # ìž„ë² ë”© API ì§€ì—°
      - alert: EmbeddingAPILatencyHigh
        expr: embedding_api_latency_ms > 5000
        for: 5m
        labels:
          severity: warning
          service: embedding-api
        annotations:
          summary: "High embedding API latency"
          description: "Embedding API latency is {{ $value }}ms, exceeding 5s threshold."

      # ì›Œì»¤ ìƒíƒœ ì•Œë¦¼
      - alert: NoActiveWorkers
        expr: rag_worker_active_jobs == 0 and rag_queue_length > 0
        for: 5m
        labels:
          severity: critical
          service: rag-worker
        annotations:
          summary: "No active RAG workers"
          description: "No active RAG workers but queue has {{ $labels.rag_queue_length }} pending jobs."
          runbook: "https://docs.ai-script-generator.com/runbooks/worker-recovery"

  - name: ai-script-generator-business
    interval: 300s  # 5ë¶„ë§ˆë‹¤ ì²´í¬
    rules:
      # UI ì˜¤ë¥˜ ê¸‰ì¦
      - alert: UIErrorSpike
        expr: increase(ui_error_panel_shown[10m]) > 50
        for: 2m
        labels:
          severity: warning
          service: frontend
        annotations:
          summary: "UI error spike detected"
          description: "{{ $value }} UI error panels shown in the last 10 minutes."

      # ì»¤ë°‹ ì„±ê³µë¥  ì €í•˜
      - alert: LowCommitSuccessRate
        expr: |
          (
            rate(commit_positive_total[30m]) / 
            (rate(commit_positive_total[30m]) + rate(commit_negative_total[30m]))
          ) * 100 < 85
        for: 10m
        labels:
          severity: warning
          service: generation
        annotations:
          summary: "Low commit success rate"
          description: "Commit success rate is {{ $value }}% over the last 30 minutes, below 85% threshold."

---
# AlertManager êµ¬ì„±
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@ai-script-generator.com'
      smtp_auth_username: 'alerts@ai-script-generator.com'
      smtp_auth_password: '${SMTP_PASSWORD}'

    templates:
      - '/etc/alertmanager/templates/*.tmpl'

    route:
      group_by: ['alertname', 'service']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'default-receiver'
      routes:
        # í¬ë¦¬í‹°ì»¬ ì•Œë¦¼ì€ ì¦‰ì‹œ ì „ì†¡
        - match:
            severity: critical
          receiver: 'critical-alerts'
          group_wait: 10s
          repeat_interval: 1h

        # ë¹„ì¦ˆë‹ˆìŠ¤ ì•Œë¦¼ì€ ìŠ¬ëž™ìœ¼ë¡œ
        - match:
            service: frontend
          receiver: 'ui-team-slack'

        # ì¸í”„ë¼ ì•Œë¦¼ì€ ìš´ì˜íŒ€ìœ¼ë¡œ
        - match:
            service: system
          receiver: 'ops-team'

    receivers:
      - name: 'default-receiver'
        email_configs:
          - to: 'devops@ai-script-generator.com'
            subject: 'ðŸš¨ {{ .GroupLabels.alertname }} - {{ .GroupLabels.service }}'
            body: |
              {{ range .Alerts }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
              {{ end }}

      - name: 'critical-alerts'
        email_configs:
          - to: 'alerts@ai-script-generator.com,cto@ai-script-generator.com'
            subject: 'ðŸ”´ CRITICAL: {{ .GroupLabels.alertname }}'
            body: |
              CRITICAL ALERT DETECTED
              
              {{ range .Alerts }}
              Service: {{ .Labels.service }}
              Alert: {{ .Annotations.summary }}
              Description: {{ .Annotations.description }}
              Runbook: {{ .Annotations.runbook }}
              Firing Time: {{ .StartsAt }}
              {{ end }}
        webhook_configs:
          - url: 'https://hooks.slack.com/services/${SLACK_CRITICAL_WEBHOOK}'
            title: 'ðŸ”´ CRITICAL: {{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

      - name: 'ui-team-slack'
        webhook_configs:
          - url: 'https://hooks.slack.com/services/${SLACK_UI_TEAM_WEBHOOK}'
            channel: '#ui-alerts'
            title: 'ðŸŽ¨ UI Alert: {{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

      - name: 'ops-team'
        webhook_configs:
          - url: 'https://hooks.slack.com/services/${SLACK_OPS_WEBHOOK}'
            channel: '#infrastructure'
            title: 'âš™ï¸ Infrastructure: {{ .GroupLabels.alertname }}'
            text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

    inhibit_rules:
      # NoActiveWorkers ì•Œë¦¼ì€ HighRAGFailureRate ì•Œë¦¼ê³¼ í•¨ê»˜ ë°œìƒí•˜ë©´ ì–µì œ
      - source_match:
          alertname: 'NoActiveWorkers'
        target_match:
          alertname: 'HighRAGFailureRate'
        equal: ['service']