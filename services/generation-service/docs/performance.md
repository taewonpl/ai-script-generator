# Generation Service ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œ

ì´ ê°€ì´ë“œëŠ” Generation Serviceì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ëª©ì°¨
1. [ì„±ëŠ¥ ëª©í‘œ ë° ì§€í‘œ](#ì„±ëŠ¥-ëª©í‘œ-ë°-ì§€í‘œ)
2. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ìµœì í™”](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜-ìµœì í™”)
3. [ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìµœì í™”](#ì• í”Œë¦¬ì¼€ì´ì…˜-ë ˆë²¨-ìµœì í™”)
4. [ìºì‹œ ìµœì í™”](#ìºì‹œ-ìµœì í™”)
5. [ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”](#ë°ì´í„°ë² ì´ìŠ¤-ìµœì í™”)
6. [ë„¤íŠ¸ì›Œí¬ ìµœì í™”](#ë„¤íŠ¸ì›Œí¬-ìµœì í™”)
7. [ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§](#ëª¨ë‹ˆí„°ë§-ë°-í”„ë¡œíŒŒì¼ë§)
8. [ìŠ¤ì¼€ì¼ë§ ì „ëµ](#ìŠ¤ì¼€ì¼ë§-ì „ëµ)

## ì„±ëŠ¥ ëª©í‘œ ë° ì§€í‘œ

### 1. í•µì‹¬ ì„±ëŠ¥ ëª©í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œ ê°’ | ì¸¡ì • ë°©ë²• |
|--------|---------|-----------|
| ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ | < 30ì´ˆ | E2E ì¸¡ì • |
| ë™ì‹œ ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ | 20ê°œ | ë¶€í•˜ í…ŒìŠ¤íŠ¸ |
| API ì‘ë‹µ ì‹œê°„ (ìºì‹œë¨) | < 100ms | P95 ì¸¡ì • |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | < 2GB | RSS ë©”ëª¨ë¦¬ |
| ìºì‹œ ì ì¤‘ë¥  | > 70% | Redis í†µê³„ |
| ì „ì²´ ì„±ê³µë¥  | > 95% | ì—ëŸ¬ìœ¨ ê¸°ë°˜ |

### 2. ì„±ëŠ¥ ì§€í‘œ ì¸¡ì •

#### ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```bash
# í˜„ì¬ ì„±ëŠ¥ ìƒíƒœ í™•ì¸
curl http://localhost:8000/api/performance/status | jq '.'

# ìƒì„¸ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ í™•ì¸
curl http://localhost:8000/api/performance/resources | jq '.'

# ì‹œìŠ¤í…œ ë¶€í•˜ í™•ì¸
curl http://localhost:8000/api/performance/load | jq '.'
```

#### ì„±ëŠ¥ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
```bash
cat > performance-validation.sh << 'EOF'
#!/bin/bash
set -e

echo "ğŸ¯ Generation Service ì„±ëŠ¥ ê²€ì¦ ì‹œì‘..."

# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
METRICS=$(curl -s http://localhost:8000/api/monitoring/metrics)
CACHE_STATUS=$(curl -s http://localhost:8000/api/cache/status)
PERFORMANCE_STATUS=$(curl -s http://localhost:8000/api/performance/status)

# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ í™•ì¸
WORKFLOW_TIME=$(echo $METRICS | jq -r '.metrics.workflow_execution_time // 0')
if (( $(echo "$WORKFLOW_TIME > 30" | bc -l) )); then
    echo "âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼: ${WORKFLOW_TIME}s (ëª©í‘œ: 30s)"
else
    echo "âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œê°„ ì–‘í˜¸: ${WORKFLOW_TIME}s"
fi

# ìºì‹œ ì ì¤‘ë¥  í™•ì¸
CACHE_HIT_RATIO=$(echo $CACHE_STATUS | jq -r '.statistics.hit_ratio // 0')
if (( $(echo "$CACHE_HIT_RATIO < 0.7" | bc -l) )); then
    echo "âŒ ìºì‹œ ì ì¤‘ë¥  ë‚®ìŒ: ${CACHE_HIT_RATIO} (ëª©í‘œ: 0.7)"
else
    echo "âœ… ìºì‹œ ì ì¤‘ë¥  ì–‘í˜¸: ${CACHE_HIT_RATIO}"
fi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
MEMORY_MB=$(echo $METRICS | jq -r '.metrics.memory_usage_mb // 0')
if (( $(echo "$MEMORY_MB > 2048" | bc -l) )); then
    echo "âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: ${MEMORY_MB}MB (ëª©í‘œ: 2048MB)"
else
    echo "âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì–‘í˜¸: ${MEMORY_MB}MB"
fi

# ì „ì²´ ì„±ê³µë¥  í™•ì¸
SUCCESS_RATE=$(echo $METRICS | jq -r '.metrics.success_rate // 1')
if (( $(echo "$SUCCESS_RATE < 0.95" | bc -l) )); then
    echo "âŒ ì„±ê³µë¥  ë‚®ìŒ: ${SUCCESS_RATE} (ëª©í‘œ: 0.95)"
else
    echo "âœ… ì„±ê³µë¥  ì–‘í˜¸: ${SUCCESS_RATE}"
fi

echo "ğŸ¯ ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ!"
EOF

chmod +x performance-validation.sh
```

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ìµœì í™”

### 1. ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤ ìµœì í™”

#### Docker ë¦¬ì†ŒìŠ¤ ì œí•œ ì„¤ì •
```yaml
# docker-compose.yml
services:
  generation-service:
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
```

#### Kubernetes ë¦¬ì†ŒìŠ¤ ìµœì í™”
```yaml
# k8s/deployment.yaml
resources:
  requests:
    memory: "1Gi"
    cpu: "500m"
  limits:
    memory: "2Gi"
    cpu: "1000m"
```

### 2. ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ìµœì í™”

#### ì„œë¹„ìŠ¤ ë¶„ë¦¬ ì „ëµ
```python
# ì„±ëŠ¥ ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬
class WorkflowExecutionService:
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì „ìš© ì„œë¹„ìŠ¤"""
    
    async def execute_workflow(self, workflow_config: Dict[str, Any]) -> Dict[str, Any]:
        # CPU ì§‘ì•½ì  ì‘ì—…
        pass

class CacheService:
    """ìºì‹± ì „ìš© ì„œë¹„ìŠ¤"""
    
    async def get_cached_result(self, key: str) -> Optional[Any]:
        # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—…
        pass
```

#### ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”
```python
# src/generation_service/optimization/async_manager.py
class AsyncManager:
    def __init__(self, config: Dict[str, Any]):
        self.pools = {
            "ai_api": asyncio.Semaphore(config.get("ai_api_concurrency", 5)),
            "io_operations": asyncio.Semaphore(config.get("io_concurrency", 20)),
            "cpu_intensive": asyncio.Semaphore(config.get("cpu_concurrency", 2))
        }
    
    async def execute_with_pool(self, pool_name: str, coro):
        async with self.pools[pool_name]:
            return await coro
```

## ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìµœì í™”

### 1. Python ì„±ëŠ¥ ìµœì í™”

#### ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
```python
# src/generation_service/optimization/memory_optimizer.py
import gc
import psutil
from typing import Optional

class MemoryOptimizer:
    def __init__(self, threshold_mb: int = 1024):
        self.threshold_mb = threshold_mb
        self.last_gc_time = time.time()
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤í–‰"""
        before_mb = self._get_memory_usage_mb()
        
        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        
        # ìˆœí™˜ ì°¸ì¡° ì •ë¦¬
        gc.collect()
        
        after_mb = self._get_memory_usage_mb()
        freed_mb = before_mb - after_mb
        
        return {
            "before_mb": before_mb,
            "after_mb": after_mb,
            "freed_mb": freed_mb,
            "objects_collected": collected
        }
    
    def _get_memory_usage_mb(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
```

#### CPU ì§‘ì•½ì  ì‘ì—… ìµœì í™”
```python
# src/generation_service/optimization/cpu_optimizer.py
import asyncio
import concurrent.futures
from functools import lru_cache

class CPUOptimizer:
    def __init__(self, max_workers: int = 4):
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
    
    async def run_cpu_intensive_task(self, func, *args, **kwargs):
        """CPU ì§‘ì•½ì  ì‘ì—…ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, func, *args, **kwargs)
    
    @lru_cache(maxsize=1000)
    def cached_computation(self, input_data: str) -> str:
        """ê³„ì‚° ê²°ê³¼ ìºì‹±"""
        # ë³µì¡í•œ ê³„ì‚° ë¡œì§
        return f"processed_{input_data}"
```

### 2. ìš”ì²­ ì²˜ë¦¬ ìµœì í™”

#### ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„
```python
# src/generation_service/optimization/batch_processor.py
import asyncio
from collections import defaultdict
from typing import List, Dict, Any

class BatchProcessor:
    def __init__(self, batch_size: int = 10, wait_time: float = 0.1):
        self.batch_size = batch_size
        self.wait_time = wait_time
        self.pending_requests = defaultdict(list)
        self.processing = False
    
    async def add_request(self, request_type: str, request_data: Any) -> Any:
        """ìš”ì²­ì„ ë°°ì¹˜ì— ì¶”ê°€í•˜ê³  ê²°ê³¼ ë°˜í™˜"""
        future = asyncio.Future()
        self.pending_requests[request_type].append((request_data, future))
        
        if not self.processing:
            asyncio.create_task(self._process_batches())
        
        return await future
    
    async def _process_batches(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        if self.processing:
            return
        
        self.processing = True
        
        try:
            await asyncio.sleep(self.wait_time)
            
            for request_type, requests in self.pending_requests.items():
                if len(requests) >= self.batch_size or len(requests) > 0:
                    await self._process_batch(request_type, requests)
                    self.pending_requests[request_type].clear()
        
        finally:
            self.processing = False
    
    async def _process_batch(self, request_type: str, requests: List[tuple]):
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš”ì²­ ì²˜ë¦¬
        data_list = [req[0] for req in requests]
        futures = [req[1] for req in requests]
        
        try:
            results = await self._execute_batch(request_type, data_list)
            for future, result in zip(futures, results):
                future.set_result(result)
        except Exception as e:
            for future in futures:
                future.set_exception(e)
```

#### ì—°ê²° í’€ë§ ìµœì í™”
```python
# src/generation_service/optimization/connection_pool.py
import aiohttp
import asyncio
from typing import Optional

class ConnectionPoolManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
    
    async def initialize(self):
        """ì—°ê²° í’€ ì´ˆê¸°í™”"""
        connector = aiohttp.TCPConnector(
            limit=self.config.get("max_connections", 100),
            limit_per_host=self.config.get("max_connections_per_host", 30),
            ttl_dns_cache=self.config.get("dns_cache_ttl", 300),
            use_dns_cache=True,
            keepalive_timeout=self.config.get("keepalive_timeout", 30),
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(
            total=self.config.get("total_timeout", 30),
            connect=self.config.get("connect_timeout", 5)
        )
        
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
    
    async def make_request(self, method: str, url: str, **kwargs) -> aiohttp.ClientResponse:
        """ìµœì í™”ëœ HTTP ìš”ì²­"""
        if not self.session:
            await self.initialize()
        
        return await self.session.request(method, url, **kwargs)
```

## ìºì‹œ ìµœì í™”

### 1. Redis ì„±ëŠ¥ íŠœë‹

#### Redis ì„¤ì • ìµœì í™”
```bash
# redis-performance.conf
# ë©”ëª¨ë¦¬ ìµœì í™”
maxmemory 1gb
maxmemory-policy allkeys-lru

# ì§€ì†ì„± ìµœì í™” (ì„±ëŠ¥ ìš°ì„ ì‹œ)
save 900 1
save 300 10
save 60 10000

# ë„¤íŠ¸ì›Œí¬ ìµœì í™”
tcp-keepalive 300
tcp-backlog 511

# ì„±ëŠ¥ ìµœì í™”
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64
```

#### Redis ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”
```bash
# Redis ì„±ëŠ¥ í†µê³„ í™•ì¸
redis-cli INFO stats | grep -E "(total_commands_processed|instantaneous_ops_per_sec|used_memory_human)"

# ìŠ¬ë¡œìš° ì¿¼ë¦¬ í™•ì¸
redis-cli SLOWLOG GET 10

# í‚¤ ë¶„í¬ í™•ì¸
redis-cli --bigkeys

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
redis-cli MEMORY USAGE key_name
```

### 2. ë‹¤ì¸µ ìºì‹œ ì „ëµ

#### L1 (ë©”ëª¨ë¦¬) + L2 (Redis) ìºì‹œ êµ¬í˜„
```python
# src/generation_service/cache/multi_level_cache.py
import asyncio
from typing import Any, Optional
from dataclasses import dataclass
import time

@dataclass
class CacheEntry:
    value: Any
    expire_time: float
    access_count: int = 0

class MultiLevelCache:
    def __init__(self, l1_size: int = 1000, l1_ttl: int = 300):
        self.l1_cache = {}  # ë©”ëª¨ë¦¬ ìºì‹œ
        self.l1_size = l1_size
        self.l1_ttl = l1_ttl
        self.redis_client = None  # Redis í´ë¼ì´ì–¸íŠ¸
    
    async def get(self, key: str) -> Optional[Any]:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        # L1 ìºì‹œ í™•ì¸
        if key in self.l1_cache:
            entry = self.l1_cache[key]
            if time.time() < entry.expire_time:
                entry.access_count += 1
                return entry.value
            else:
                del self.l1_cache[key]
        
        # L2 (Redis) ìºì‹œ í™•ì¸
        if self.redis_client:
            value = await self.redis_client.get(key)
            if value is not None:
                # L1 ìºì‹œì— ìŠ¹ê²©
                await self._set_l1(key, value)
                return value
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600):
        """ë‹¤ì¸µ ìºì‹œì— ê°’ ì €ì¥"""
        # L1 ìºì‹œì— ì €ì¥
        await self._set_l1(key, value)
        
        # L2 (Redis) ìºì‹œì— ì €ì¥
        if self.redis_client:
            await self.redis_client.setex(key, ttl, value)
    
    async def _set_l1(self, key: str, value: Any):
        """L1 ìºì‹œì— ê°’ ì €ì¥"""
        if len(self.l1_cache) >= self.l1_size:
            # LRU ì •ì±…ìœ¼ë¡œ ì œê±°
            oldest_key = min(
                self.l1_cache.keys(),
                key=lambda k: self.l1_cache[k].access_count
            )
            del self.l1_cache[oldest_key]
        
        self.l1_cache[key] = CacheEntry(
            value=value,
            expire_time=time.time() + self.l1_ttl
        )
```

### 3. ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì „ëµ

#### ì˜ˆì¸¡ ê¸°ë°˜ ìºì‹œ ì›Œë°
```python
# src/generation_service/cache/smart_warmer.py
import asyncio
from typing import List, Dict, Any
from collections import defaultdict

class SmartCacheWarmer:
    def __init__(self, cache_manager):
        self.cache_manager = cache_manager
        self.access_patterns = defaultdict(int)
        self.warming_in_progress = set()
    
    async def record_access(self, cache_key: str):
        """ìºì‹œ ì ‘ê·¼ íŒ¨í„´ ê¸°ë¡"""
        self.access_patterns[cache_key] += 1
        
        # ì„ê³„ê°’ ì´ˆê³¼ì‹œ ê´€ë ¨ í‚¤ë“¤ ì˜ˆì¸¡ ì›Œë°
        if self.access_patterns[cache_key] > 10:
            await self._predictive_warm(cache_key)
    
    async def _predictive_warm(self, hot_key: str):
        """ì˜ˆì¸¡ ê¸°ë°˜ ìºì‹œ ì›Œë°"""
        if hot_key in self.warming_in_progress:
            return
        
        self.warming_in_progress.add(hot_key)
        
        try:
            # ê´€ë ¨ í‚¤ íŒ¨í„´ ì˜ˆì¸¡
            related_keys = self._predict_related_keys(hot_key)
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì›Œë°
            warming_tasks = [
                self._warm_key(key) for key in related_keys
                if key not in self.cache_manager.cache
            ]
            
            if warming_tasks:
                await asyncio.gather(*warming_tasks, return_exceptions=True)
        
        finally:
            self.warming_in_progress.discard(hot_key)
    
    def _predict_related_keys(self, key: str) -> List[str]:
        """ê´€ë ¨ í‚¤ ì˜ˆì¸¡ ë¡œì§"""
        # íŒ¨í„´ ê¸°ë°˜ ê´€ë ¨ í‚¤ ìƒì„±
        if "prompt_result" in key:
            # ë¹„ìŠ·í•œ í”„ë¡¬í”„íŠ¸ í‚¤ë“¤ ë°˜í™˜
            base_pattern = key.split(":")[0] + ":"
            return [
                f"{base_pattern}similar_1",
                f"{base_pattern}similar_2"
            ]
        
        return []
```

## ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 1. ì—°ê²° í’€ ìµœì í™”

#### PostgreSQL ì—°ê²° í’€ ì„¤ì •
```python
# src/generation_service/database/connection_pool.py
import asyncpg
import asyncio
from typing import Optional

class DatabasePool:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.pool: Optional[asyncpg.Pool] = None
    
    async def initialize(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ì´ˆê¸°í™”"""
        self.pool = await asyncpg.create_pool(
            host=self.config["host"],
            port=self.config["port"],
            user=self.config["user"],
            password=self.config["password"],
            database=self.config["database"],
            min_size=self.config.get("min_connections", 5),
            max_size=self.config.get("max_connections", 20),
            max_queries=self.config.get("max_queries", 50000),
            max_inactive_connection_lifetime=self.config.get("max_idle_time", 300),
            command_timeout=self.config.get("command_timeout", 30)
        )
    
    async def execute_query(self, query: str, *args) -> List[Dict[str, Any]]:
        """ìµœì í™”ëœ ì¿¼ë¦¬ ì‹¤í–‰"""
        async with self.pool.acquire() as connection:
            # ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            
            try:
                result = await connection.fetch(query, *args)
                execution_time = time.time() - start_time
                
                # ìŠ¬ë¡œìš° ì¿¼ë¦¬ ë¡œê¹…
                if execution_time > 1.0:
                    logger.warning(f"Slow query detected: {execution_time:.2f}s - {query[:100]}")
                
                return [dict(row) for row in result]
            
            except Exception as e:
                logger.error(f"Query failed: {query[:100]} - {str(e)}")
                raise
```

### 2. ì¿¼ë¦¬ ìµœì í™”

#### ì¸ë±ìŠ¤ ìµœì í™” ì „ëµ
```sql
-- ì„±ëŠ¥ í¬ë¦¬í‹°ì»¬ ì¿¼ë¦¬ë¥¼ ìœ„í•œ ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_workflows_status_created 
ON workflows(status, created_at) 
WHERE status IN ('running', 'pending');

-- ë³µí•© ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX CONCURRENTLY idx_cache_entries_type_key 
ON cache_entries(cache_type, cache_key, expires_at);

-- íŒŒí‹°ì…”ë‹ì„ ìœ„í•œ ì¤€ë¹„
CREATE INDEX CONCURRENTLY idx_metrics_timestamp 
ON performance_metrics(timestamp DESC);
```

#### ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
# src/generation_service/database/query_monitor.py
import time
import asyncio
from collections import defaultdict
from typing import Dict, List

class QueryPerformanceMonitor:
    def __init__(self):
        self.query_stats = defaultdict(list)
        self.slow_query_threshold = 1.0
    
    def record_query(self, query: str, execution_time: float):
        """ì¿¼ë¦¬ ì„±ëŠ¥ ê¸°ë¡"""
        self.query_stats[query].append(execution_time)
        
        if execution_time > self.slow_query_threshold:
            logger.warning(f"Slow query: {execution_time:.2f}s - {query[:100]}")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {}
        
        for query, times in self.query_stats.items():
            if times:
                report[query[:100]] = {
                    "count": len(times),
                    "avg_time": sum(times) / len(times),
                    "max_time": max(times),
                    "min_time": min(times)
                }
        
        return report
```

## ë„¤íŠ¸ì›Œí¬ ìµœì í™”

### 1. HTTP/2 ë° ì—°ê²° ìµœì í™”

#### Nginx HTTP/2 ì„¤ì •
```nginx
# nginx/nginx-performance.conf
server {
    listen 443 ssl http2;
    server_name api.yourdomain.com;
    
    # HTTP/2 ìµœì í™”
    http2_push_preload on;
    http2_max_field_size 16k;
    http2_max_header_size 32k;
    
    # ì—°ê²° ìµœì í™”
    keepalive_timeout 65;
    keepalive_requests 1000;
    
    # ì••ì¶• ìµœì í™”
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_comp_level 6;
    gzip_types
        application/json
        application/javascript
        text/css
        text/plain
        text/xml;
    
    # ë²„í¼ë§ ìµœì í™”
    client_body_buffer_size 128k;
    client_max_body_size 50m;
    proxy_buffering on;
    proxy_buffer_size 4k;
    proxy_buffers 8 4k;
    
    location /api/ {
        proxy_pass http://generation_service;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        
        # íƒ€ì„ì•„ì›ƒ ìµœì í™”
        proxy_connect_timeout 5s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
        
        # ìºì‹± í—¤ë”
        add_header Cache-Control "public, max-age=300" always;
    }
}
```

### 2. CDN ë° ì •ì  ìì‚° ìµœì í™”

#### ì •ì  ìì‚° ìºì‹± ì „ëµ
```nginx
# ì •ì  íŒŒì¼ ìµœì í™”
location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
    expires 1y;
    add_header Cache-Control "public, immutable";
    add_header Vary Accept-Encoding;
    
    # ì••ì¶•
    gzip_static on;
    brotli_static on;
}

# API ì‘ë‹µ ìºì‹±
location /api/monitoring/ {
    proxy_pass http://generation_service;
    proxy_cache api_cache;
    proxy_cache_valid 200 5m;
    proxy_cache_key "$request_uri";
    add_header X-Cache-Status $upstream_cache_status;
}
```

## ëª¨ë‹ˆí„°ë§ ë° í”„ë¡œíŒŒì¼ë§

### 1. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

#### ì»¤ìŠ¤í…€ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# src/generation_service/monitoring/performance_collector.py
import time
import asyncio
from dataclasses import dataclass, field
from typing import Dict, List, Any
import psutil

@dataclass
class PerformanceSnapshot:
    timestamp: float
    cpu_percent: float
    memory_mb: float
    active_connections: int
    request_rate: float
    response_time_p95: float
    cache_hit_ratio: float
    error_rate: float

class PerformanceCollector:
    def __init__(self):
        self.snapshots: List[PerformanceSnapshot] = []
        self.max_snapshots = 1000
        self.collection_interval = 10.0
        self.collecting = False
    
    async def start_collection(self):
        """ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"""
        if self.collecting:
            return
        
        self.collecting = True
        asyncio.create_task(self._collection_loop())
    
    async def _collection_loop(self):
        """ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ ë£¨í”„"""
        while self.collecting:
            try:
                snapshot = await self._collect_snapshot()
                self.snapshots.append(snapshot)
                
                # ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì œê±°
                if len(self.snapshots) > self.max_snapshots:
                    self.snapshots = self.snapshots[-self.max_snapshots:]
                
                await asyncio.sleep(self.collection_interval)
            
            except Exception as e:
                logger.error(f"Performance collection error: {e}")
                await asyncio.sleep(self.collection_interval)
    
    async def _collect_snapshot(self) -> PerformanceSnapshot:
        """ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· ìˆ˜ì§‘"""
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
        cpu_percent = psutil.cpu_percent()
        memory_info = psutil.virtual_memory()
        memory_mb = memory_info.used / 1024 / 1024
        
        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ (ê°€ì •)
        active_connections = await self._get_active_connections()
        request_rate = await self._get_request_rate()
        response_time_p95 = await self._get_response_time_p95()
        cache_hit_ratio = await self._get_cache_hit_ratio()
        error_rate = await self._get_error_rate()
        
        return PerformanceSnapshot(
            timestamp=time.time(),
            cpu_percent=cpu_percent,
            memory_mb=memory_mb,
            active_connections=active_connections,
            request_rate=request_rate,
            response_time_p95=response_time_p95,
            cache_hit_ratio=cache_hit_ratio,
            error_rate=error_rate
        )
    
    def get_performance_trend(self, minutes: int = 60) -> Dict[str, Any]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        cutoff_time = time.time() - (minutes * 60)
        recent_snapshots = [
            s for s in self.snapshots 
            if s.timestamp > cutoff_time
        ]
        
        if not recent_snapshots:
            return {"error": "No recent data available"}
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        avg_cpu = sum(s.cpu_percent for s in recent_snapshots) / len(recent_snapshots)
        avg_memory = sum(s.memory_mb for s in recent_snapshots) / len(recent_snapshots)
        avg_response_time = sum(s.response_time_p95 for s in recent_snapshots) / len(recent_snapshots)
        
        return {
            "period_minutes": minutes,
            "snapshot_count": len(recent_snapshots),
            "avg_cpu_percent": round(avg_cpu, 2),
            "avg_memory_mb": round(avg_memory, 2),
            "avg_response_time_p95": round(avg_response_time, 3),
            "current_performance_rating": self._calculate_performance_rating(recent_snapshots[-1])
        }
    
    def _calculate_performance_rating(self, snapshot: PerformanceSnapshot) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        score = 0
        
        # CPU ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        if snapshot.cpu_percent < 50:
            score += 25
        elif snapshot.cpu_percent < 75:
            score += 15
        elif snapshot.cpu_percent < 90:
            score += 5
        
        # ë©”ëª¨ë¦¬ ì ìˆ˜ (2GB ê¸°ì¤€)
        if snapshot.memory_mb < 1024:
            score += 25
        elif snapshot.memory_mb < 1536:
            score += 15
        elif snapshot.memory_mb < 2048:
            score += 5
        
        # ì‘ë‹µ ì‹œê°„ ì ìˆ˜
        if snapshot.response_time_p95 < 0.1:
            score += 25
        elif snapshot.response_time_p95 < 0.5:
            score += 15
        elif snapshot.response_time_p95 < 1.0:
            score += 5
        
        # ìºì‹œ ì ì¤‘ë¥  ì ìˆ˜
        if snapshot.cache_hit_ratio > 0.8:
            score += 25
        elif snapshot.cache_hit_ratio > 0.7:
            score += 15
        elif snapshot.cache_hit_ratio > 0.5:
            score += 5
        
        # ë“±ê¸‰ ê²°ì •
        if score >= 80:
            return "excellent"
        elif score >= 60:
            return "good"
        elif score >= 40:
            return "fair"
        else:
            return "poor"
```

### 2. ìë™í™”ëœ ì„±ëŠ¥ ìµœì í™”

#### ì ì‘í˜• ìµœì í™” ì‹œìŠ¤í…œ
```python
# src/generation_service/optimization/adaptive_optimizer.py
import asyncio
from typing import Dict, Any, List
from dataclasses import dataclass

@dataclass
class OptimizationRule:
    condition: str
    action: str
    threshold: float
    cooldown_seconds: int = 300

class AdaptiveOptimizer:
    def __init__(self, performance_collector):
        self.performance_collector = performance_collector
        self.optimization_rules = [
            OptimizationRule("memory_mb", "gc_collect", 1536),
            OptimizationRule("cpu_percent", "reduce_workers", 80),
            OptimizationRule("response_time_p95", "clear_cache", 2.0),
            OptimizationRule("cache_hit_ratio", "warm_cache", 0.6, 600)
        ]
        self.last_optimization_time = {}
        self.optimizing = False
    
    async def start_optimization(self):
        """ì ì‘í˜• ìµœì í™” ì‹œì‘"""
        if self.optimizing:
            return
        
        self.optimizing = True
        asyncio.create_task(self._optimization_loop())
    
    async def _optimization_loop(self):
        """ìµœì í™” ë£¨í”„"""
        while self.optimizing:
            try:
                await self._check_and_optimize()
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ í™•ì¸
            
            except Exception as e:
                logger.error(f"Adaptive optimization error: {e}")
                await asyncio.sleep(60)
    
    async def _check_and_optimize(self):
        """ì„±ëŠ¥ í™•ì¸ ë° ìµœì í™” ì‹¤í–‰"""
        if not self.performance_collector.snapshots:
            return
        
        latest_snapshot = self.performance_collector.snapshots[-1]
        current_time = time.time()
        
        for rule in self.optimization_rules:
            # ì¿¨ë‹¤ìš´ í™•ì¸
            last_optimization = self.last_optimization_time.get(rule.action, 0)
            if current_time - last_optimization < rule.cooldown_seconds:
                continue
            
            # ì¡°ê±´ í™•ì¸
            metric_value = getattr(latest_snapshot, rule.condition, 0)
            should_optimize = False
            
            if rule.condition in ["memory_mb", "cpu_percent", "response_time_p95"]:
                should_optimize = metric_value > rule.threshold
            elif rule.condition == "cache_hit_ratio":
                should_optimize = metric_value < rule.threshold
            
            if should_optimize:
                await self._execute_optimization(rule)
                self.last_optimization_time[rule.action] = current_time
    
    async def _execute_optimization(self, rule: OptimizationRule):
        """ìµœì í™” ì‹¤í–‰"""
        logger.info(f"Executing adaptive optimization: {rule.action} (condition: {rule.condition} threshold: {rule.threshold})")
        
        try:
            if rule.action == "gc_collect":
                await self._force_garbage_collection()
            elif rule.action == "reduce_workers":
                await self._reduce_worker_count()
            elif rule.action == "clear_cache":
                await self._clear_expired_cache()
            elif rule.action == "warm_cache":
                await self._warm_popular_cache()
        
        except Exception as e:
            logger.error(f"Optimization action failed: {rule.action} - {e}")
    
    async def _force_garbage_collection(self):
        """ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"""
        import gc
        collected = gc.collect()
        logger.info(f"Garbage collection completed: {collected} objects collected")
    
    async def _reduce_worker_count(self):
        """ì›Œì»¤ ìˆ˜ ê°ì†Œ"""
        # ì›Œì»¤ ìˆ˜ ì¡°ì • ë¡œì§
        logger.info("Reducing worker count due to high CPU usage")
    
    async def _clear_expired_cache(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        # ìºì‹œ ì •ë¦¬ ë¡œì§
        logger.info("Clearing expired cache entries due to slow response time")
    
    async def _warm_popular_cache(self):
        """ì¸ê¸° ìºì‹œ ì›Œë°"""
        # ìºì‹œ ì›Œë° ë¡œì§
        logger.info("Warming popular cache entries due to low hit ratio")
```

## ìŠ¤ì¼€ì¼ë§ ì „ëµ

### 1. ìˆ˜í‰ ìŠ¤ì¼€ì¼ë§

#### Kubernetes HPA ì„¤ì •
```yaml
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: generation-service-hpa
  namespace: generation-service
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: generation-service
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 60
      selectPolicy: Max
```

#### ìˆ˜ì§ ìŠ¤ì¼€ì¼ë§ (VPA)
```yaml
# k8s/vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: generation-service-vpa
  namespace: generation-service
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: generation-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: generation-service
      minAllowed:
        cpu: 100m
        memory: 512Mi
      maxAllowed:
        cpu: 2000m
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

### 2. ë¡œë“œ ë°¸ëŸ°ì‹± ìµœì í™”

#### ì§€ëŠ¥í˜• ë¡œë“œ ë°¸ëŸ°ì‹±
```nginx
# nginx/upstream-optimization.conf
upstream generation_service {
    least_conn;
    
    # ì„œë²„ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
    server generation-service-1:8000 weight=3 max_fails=2 fail_timeout=30s;
    server generation-service-2:8000 weight=2 max_fails=2 fail_timeout=30s;
    server generation-service-3:8000 weight=1 max_fails=2 fail_timeout=30s;
    
    # ì—°ê²° ìœ ì§€
    keepalive 32;
    keepalive_requests 1000;
    keepalive_timeout 60s;
}
```

### 3. ì„±ëŠ¥ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§

#### ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
```python
# src/generation_service/scaling/custom_scaler.py
import asyncio
import time
from typing import Dict, Any

class CustomAutoScaler:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.scaling_enabled = True
        self.last_scale_time = 0
        self.scale_cooldown = config.get("scale_cooldown", 300)  # 5ë¶„
    
    async def evaluate_scaling(self) -> Dict[str, Any]:
        """ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± í‰ê°€"""
        current_time = time.time()
        
        # ì¿¨ë‹¤ìš´ í™•ì¸
        if current_time - self.last_scale_time < self.scale_cooldown:
            return {"action": "none", "reason": "cooldown"}
        
        # í˜„ì¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        metrics = await self._collect_scaling_metrics()
        
        # ìŠ¤ì¼€ì¼ë§ ê²°ì •
        decision = self._make_scaling_decision(metrics)
        
        if decision["action"] != "none":
            self.last_scale_time = current_time
        
        return decision
    
    async def _collect_scaling_metrics(self) -> Dict[str, float]:
        """ìŠ¤ì¼€ì¼ë§ ê²°ì •ì„ ìœ„í•œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë¡œì§
        return {
            "avg_response_time": 0.5,
            "requests_per_second": 150,
            "cpu_utilization": 65,
            "memory_utilization": 70,
            "queue_length": 25,
            "error_rate": 0.02
        }
    
    def _make_scaling_decision(self, metrics: Dict[str, float]) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§ ê²°ì •"""
        scale_up_score = 0
        scale_down_score = 0
        
        # ìŠ¤ì¼€ì¼ ì—… ì¡°ê±´ í‰ê°€
        if metrics["avg_response_time"] > 1.0:
            scale_up_score += 30
        if metrics["cpu_utilization"] > 80:
            scale_up_score += 25
        if metrics["memory_utilization"] > 85:
            scale_up_score += 25
        if metrics["queue_length"] > 50:
            scale_up_score += 20
        
        # ìŠ¤ì¼€ì¼ ë‹¤ìš´ ì¡°ê±´ í‰ê°€
        if metrics["avg_response_time"] < 0.2:
            scale_down_score += 20
        if metrics["cpu_utilization"] < 30:
            scale_down_score += 25
        if metrics["memory_utilization"] < 40:
            scale_down_score += 25
        if metrics["requests_per_second"] < 50:
            scale_down_score += 30
        
        # ê²°ì • ë¡œì§
        if scale_up_score > 50:
            return {
                "action": "scale_up",
                "reason": f"High load detected (score: {scale_up_score})",
                "target_replicas": "+2"
            }
        elif scale_down_score > 60:
            return {
                "action": "scale_down",
                "reason": f"Low load detected (score: {scale_down_score})",
                "target_replicas": "-1"
            }
        else:
            return {
                "action": "none",
                "reason": "Metrics within acceptable range"
            }
```

ì´ ì„±ëŠ¥ íŠœë‹ ê°€ì´ë“œë¥¼ ë”°ë¼ Generation Serviceì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ì ì§„ì ì¸ íŠœë‹ì„ í†µí•´ ìµœìƒì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ìœ ì§€í•˜ì„¸ìš”.