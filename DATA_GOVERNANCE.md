# ğŸ“‹ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì •ì±… ë° êµ¬í˜„

## ğŸ”„ 1. Embed Version ê´€ë¦¬

### ë²„ì „ ê´€ë¦¬ ì²´ê³„

```python
# í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ë²„ì „ ê´€ë¦¬
RAG_EMBED_VERSION = os.getenv("RAG_EMBED_VERSION", "v1.0")

# ì§€ì›ë˜ëŠ” ë²„ì „ í˜•ì‹:
# - ì˜ë¯¸ë¡ ì  ë²„ì „: v1.0, v1.1, v2.0
# - íƒ€ì„ìŠ¤íƒ¬í”„: v20250828, v2025-08-28
# - ëª¨ë¸ ì‹ë³„: ada-002-v1, text-3-large-v1
```

### Reindex All ê¸°ëŠ¥ êµ¬í˜„

```python
# API ì—”ë“œí¬ì¸íŠ¸: POST /rag/reindex-all
@router.post("/reindex-all")
async def reindex_all_documents(request: ReindexRequest):
    """
    ëª¨ë“  ë¬¸ì„œë¥¼ ìƒˆë¡œìš´ ì„ë² ë”© ë²„ì „ìœ¼ë¡œ ì¬ìƒ‰ì¸
    - ë°°ì¹˜ ì²˜ë¦¬: ê¸°ë³¸ 10ê°œì”© ì²˜ë¦¬
    - ì ì§„ì  ì—…ë°ì´íŠ¸: ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ìŒ
    - ë¡¤ë°± ì§€ì›: ì´ì „ ë²„ì „ ìœ ì§€
    """
    
    # 1. ë²„ì „ ìœ íš¨ì„± ê²€ì‚¬
    if request.new_embed_version == current_version:
        raise HTTPException(400, "Same version as current")
    
    # 2. ë°°ì¹˜ë³„ ì¬ìƒ‰ì¸ ì‘ì—… íì‰
    for batch in chunk_documents(project_documents, request.batch_size):
        job = await worker_adapter.enqueue_reindex_all(
            project_id=request.project_id,
            document_ids=batch,
            old_embed_version=current_version,
            new_embed_version=request.new_embed_version
        )
```

### ë²„ì „ ì´ê´€ ì „ëµ

#### ë¸”ë£¨-ê·¸ë¦° ë°°í¬ íŒ¨í„´:
```python
class EmbedVersionManager:
    """ì„ë² ë”© ë²„ì „ ì´ê´€ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.current_version = os.getenv("RAG_EMBED_VERSION", "v1.0")
        self.target_version = None
        self.migration_status = "idle"  # idle, preparing, migrating, rollback
    
    async def start_migration(self, target_version: str):
        """ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘"""
        self.target_version = target_version
        self.migration_status = "preparing"
        
        # 1. ìƒˆ ë²„ì „ìœ¼ë¡œ ì¸ë±ìŠ¤ ìƒì„±
        await self._create_new_index(target_version)
        
        # 2. ë¬¸ì„œ ë°°ì¹˜ë³„ ì´ê´€
        await self._migrate_documents_batch()
        
        # 3. ê²€ì¦ ë° ì „í™˜
        await self._validate_and_switch()
    
    async def rollback_migration(self):
        """ì´ì „ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±"""
        if self.migration_status != "migrating":
            raise ValueError("No active migration to rollback")
        
        # ìƒˆ ì¸ë±ìŠ¤ ì‚­ì œ, ì´ì „ ë²„ì „ìœ¼ë¡œ ë³µì›
        await self._cleanup_failed_migration()
        self.migration_status = "rollback"
```

#### ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥:
```python
# ì´ê´€ ì¤‘ ë°ì´í„° ì¼ê´€ì„± ì²´í¬
async def verify_migration_integrity(old_version: str, new_version: str):
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¬´ê²°ì„± ê²€ì¦"""
    
    # 1. ë¬¸ì„œ ìˆ˜ ì¼ì¹˜ í™•ì¸
    old_count = await count_documents_by_version(old_version)
    new_count = await count_documents_by_version(new_version)
    
    if old_count != new_count:
        raise IntegrityError(f"Document count mismatch: {old_count} != {new_count}")
    
    # 2. ì„ë² ë”© í’ˆì§ˆ ê²€ì¦ (ìƒ˜í”Œë§)
    sample_docs = await get_sample_documents(100)
    for doc in sample_docs:
        old_embedding = await get_embedding(doc.id, old_version)
        new_embedding = await get_embedding(doc.id, new_version)
        
        # ë²¡í„° ìœ ì‚¬ë„ ì„ê³„ê°’ ê²€ì‚¬ (0.8 ì´ìƒ ìœ ì§€ë˜ì–´ì•¼ í•¨)
        similarity = cosine_similarity(old_embedding, new_embedding)
        if similarity < 0.8:
            logger.warning(f"Low similarity for doc {doc.id}: {similarity}")
```

---

## ğŸ—‘ï¸ 2. ë¬¸ì„œ ì‚­ì œ ì‹œ ë²¡í„° ìŠ¤í† ì–´ ì—°ì‡„ ì‚­ì œ

### ì—°ì‡„ ì‚­ì œ ë³´ì¥ ë©”ì»¤ë‹ˆì¦˜

```python
# services/generation-service/src/generation_service/services/rag_cleanup.py

class VectorStoreCascadeDeleteManager:
    """ë²¡í„° ìŠ¤í† ì–´ ì—°ì‡„ ì‚­ì œ ê´€ë¦¬ì"""
    
    def __init__(self, chroma_client, db_session):
        self.chroma = chroma_client
        self.db = db_session
        self.deletion_log = []
    
    async def delete_document_cascade(self, document_id: str, user_id: str):
        """ë¬¸ì„œ ë° ê´€ë ¨ ë²¡í„° ì™„ì „ ì‚­ì œ"""
        
        deletion_context = {
            'document_id': document_id,
            'user_id': user_id,
            'timestamp': datetime.utcnow(),
            'trace_id': f"delete-{document_id}-{uuid4().hex[:8]}"
        }
        
        try:
            # 1. ì‚­ì œ ì „ ë°±ì—… (ê°ì‚¬ ëª©ì )
            await self._backup_before_deletion(document_id, deletion_context)
            
            # 2. ChromaDBì—ì„œ ë²¡í„° ì‚­ì œ
            await self._delete_from_vector_store(document_id, deletion_context)
            
            # 3. ë©”íƒ€ë°ì´í„° DBì—ì„œ ì‚­ì œ
            await self._delete_from_metadata_db(document_id, deletion_context)
            
            # 4. ìºì‹œ ë¬´íš¨í™”
            await self._invalidate_caches(document_id)
            
            # 5. ê°ì‚¬ ë¡œê·¸ ê¸°ë¡
            await self._log_deletion(deletion_context, success=True)
            
        except Exception as e:
            # ë¶€ë¶„ ì‚­ì œ ë³µêµ¬
            await self._handle_deletion_failure(deletion_context, e)
            raise
    
    async def _delete_from_vector_store(self, document_id: str, context: dict):
        """ChromaDBì—ì„œ ë²¡í„° ì‚­ì œ"""
        
        # 1. ê´€ë ¨ ì²­í¬ ID ì¡°íšŒ
        chunk_ids = await self._get_chunk_ids(document_id)
        
        if not chunk_ids:
            logger.warning(f"No chunks found for document {document_id}")
            return
        
        # 2. ChromaDB ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
        try:
            collection = await self.chroma.get_collection("documents")
            collection.delete(ids=chunk_ids)
            
            logger.info(f"Deleted {len(chunk_ids)} chunks from ChromaDB", extra={
                'document_id': document_id,
                'chunk_count': len(chunk_ids),
                'trace_id': context['trace_id']
            })
            
        except Exception as e:
            raise VectorStoreDeletionError(f"Failed to delete from ChromaDB: {e}")
    
    async def _verify_complete_deletion(self, document_id: str):
        """ì‚­ì œ ì™„ë£Œ ê²€ì¦"""
        
        # 1. ChromaDB ê²€ì¦
        try:
            collection = await self.chroma.get_collection("documents")
            remaining_chunks = collection.get(
                where={"document_id": document_id}
            )
            
            if remaining_chunks['ids']:
                raise DeletionVerificationError(
                    f"Found {len(remaining_chunks['ids'])} remaining chunks in ChromaDB"
                )
                
        except Exception as e:
            logger.error(f"ChromaDB verification failed: {e}")
            raise
        
        # 2. ë©”íƒ€ë°ì´í„° DB ê²€ì¦
        doc = self.db.query(RAGDocumentDB).filter(
            RAGDocumentDB.id == document_id
        ).first()
        
        if doc:
            raise DeletionVerificationError(f"Document still exists in metadata DB")
        
        logger.info(f"Deletion verification passed for document {document_id}")
```

### ì‚­ì œ ì‹¤íŒ¨ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

```python
class DeletionFailureRecovery:
    """ì‚­ì œ ì‹¤íŒ¨ ì‹œ ë³µêµ¬ ì²˜ë¦¬"""
    
    async def handle_partial_deletion(self, document_id: str, failure_point: str):
        """ë¶€ë¶„ ì‚­ì œ ìƒíƒœ ë³µêµ¬"""
        
        if failure_point == "vector_store":
            # ë©”íƒ€ë°ì´í„°ëŠ” ì‚­ì œë˜ì—ˆì§€ë§Œ ë²¡í„°ëŠ” ë‚¨ì•„ìˆëŠ” ê²½ìš°
            await self._cleanup_orphaned_vectors(document_id)
            
        elif failure_point == "metadata_db":
            # ë²¡í„°ëŠ” ì‚­ì œë˜ì—ˆì§€ë§Œ ë©”íƒ€ë°ì´í„°ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°
            await self._restore_vector_from_backup(document_id)
            
        elif failure_point == "cache":
            # ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨ - ê°•ì œ ë§Œë£Œ
            await self._force_cache_expiry(document_id)
    
    async def schedule_cleanup_job(self, failed_deletions: List[str]):
        """ì‹¤íŒ¨í•œ ì‚­ì œ ì‘ì—… ì¬ì‹œë„ ìŠ¤ì¼€ì¤„ë§"""
        
        for document_id in failed_deletions:
            cleanup_job = {
                'job_type': 'cascade_delete_retry',
                'document_id': document_id,
                'scheduled_at': datetime.utcnow() + timedelta(hours=1),
                'max_retries': 3
            }
            
            await self.worker_adapter.enqueue_cleanup_job(cleanup_job)
```

---

## ğŸ“… 3. 180ì¼ ë³´ê´€/ì‚­ì œ ì •ì±…

### ë°ì´í„° ë³´ê´€ ì •ì±… êµ¬í˜„

```python
# services/generation-service/src/generation_service/data_retention.py

class DataRetentionManager:
    """ë°ì´í„° ë³´ê´€ ì •ì±… ê´€ë¦¬ì"""
    
    # ë³´ê´€ ê¸°ê°„ ì„¤ì •
    RETENTION_POLICIES = {
        'user_documents': timedelta(days=180),      # ì‚¬ìš©ì ë¬¸ì„œ
        'rag_embeddings': timedelta(days=180),      # RAG ì„ë² ë”©
        'job_logs': timedelta(days=30),             # ì‘ì—… ë¡œê·¸
        'audit_logs': timedelta(days=365),          # ê°ì‚¬ ë¡œê·¸
        'error_logs': timedelta(days=90),           # ì˜¤ë¥˜ ë¡œê·¸
        'user_sessions': timedelta(days=7),         # ì‚¬ìš©ì ì„¸ì…˜
    }
    
    def __init__(self):
        self.cleanup_enabled = os.getenv("DATA_RETENTION_ENABLED", "true").lower() == "true"
        self.dry_run = os.getenv("RETENTION_DRY_RUN", "false").lower() == "true"
    
    async def execute_retention_policy(self):
        """ë³´ê´€ ì •ì±… ì‹¤í–‰ (ì¼ì¼ ìŠ¤ì¼€ì¤„)"""
        
        if not self.cleanup_enabled:
            logger.info("Data retention is disabled")
            return
        
        logger.info("Starting daily data retention cleanup")
        
        cleanup_results = {}
        
        for data_type, retention_period in self.RETENTION_POLICIES.items():
            try:
                cutoff_date = datetime.utcnow() - retention_period
                result = await self._cleanup_data_type(data_type, cutoff_date)
                cleanup_results[data_type] = result
                
            except Exception as e:
                logger.error(f"Failed to cleanup {data_type}: {e}")
                cleanup_results[data_type] = {'error': str(e)}
        
        # ì •ë¦¬ ê²°ê³¼ ë³´ê³ 
        await self._report_cleanup_results(cleanup_results)
    
    async def _cleanup_user_documents(self, cutoff_date: datetime):
        """ì‚¬ìš©ì ë¬¸ì„œ ì •ë¦¬"""
        
        # 1. ë§Œë£Œëœ ë¬¸ì„œ ì¡°íšŒ
        expired_docs = await self._get_expired_documents(cutoff_date)
        
        if not expired_docs:
            return {'deleted': 0, 'message': 'No expired documents found'}
        
        # 2. ì‚¬ìš©ìë³„ ê·¸ë£¹í™”
        docs_by_user = {}
        for doc in expired_docs:
            user_id = doc.user_id or 'anonymous'
            if user_id not in docs_by_user:
                docs_by_user[user_id] = []
            docs_by_user[user_id].append(doc)
        
        # 3. ì‚¬ìš©ìë³„ ì•Œë¦¼ ë° ì‚­ì œ
        deleted_count = 0
        for user_id, user_docs in docs_by_user.items():
            
            # ì‚¬ìš©ìì—ê²Œ ì‚­ì œ ì˜ˆì • ì•Œë¦¼ (7ì¼ ì „)
            if self._should_notify_user(user_docs[0].created_at):
                await self._notify_user_before_deletion(user_id, user_docs)
                continue  # ì•Œë¦¼ í›„ 7ì¼ ë” ëŒ€ê¸°
            
            # ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰
            for doc in user_docs:
                if not self.dry_run:
                    await self.cascade_delete_manager.delete_document_cascade(doc.id, user_id)
                deleted_count += 1
        
        return {
            'deleted': deleted_count,
            'dry_run': self.dry_run,
            'users_affected': len(docs_by_user)
        }
```

### ê´€ë¦¬ì ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸

```python
# services/generation-service/src/generation_service/api/admin.py

@router.delete("/analytics/erase", dependencies=[Depends(verify_admin_token)])
async def admin_erase_data(
    request: AdminEraseRequest,
    current_admin: dict = Depends(get_current_admin)
):
    """
    ê´€ë¦¬ì ë°ì´í„° ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸
    - ë²•ì  ìš”ì²­ (GDPR, ê°œì¸ì •ë³´ë³´í˜¸ë²•) ëŒ€ì‘
    - ì™„ì „ ì‚­ì œ ë³´ì¥
    - ê°ì‚¬ ë¡œê·¸ ìœ ì§€
    """
    
    # 1. ê¶Œí•œ ê²€ì¦
    if not current_admin.get('can_delete_user_data'):
        raise HTTPException(403, "Insufficient permissions for data deletion")
    
    # 2. ìš”ì²­ ìœ íš¨ì„± ê²€ì‚¬
    if not request.user_id and not request.document_ids:
        raise HTTPException(400, "Either user_id or document_ids must be provided")
    
    # 3. ì‚­ì œ ë²”ìœ„ ê³„ì‚°
    deletion_scope = await calculate_deletion_scope(request)
    
    # 4. ê´€ë¦¬ì ìŠ¹ì¸ ë¡œê·¸
    approval_log = {
        'admin_id': current_admin['id'],
        'admin_email': current_admin['email'],
        'deletion_type': request.deletion_type,
        'reason': request.reason,
        'legal_basis': request.legal_basis,
        'scope': deletion_scope,
        'timestamp': datetime.utcnow(),
        'ip_address': request.client.host if hasattr(request, 'client') else None
    }
    
    await log_admin_action(approval_log)
    
    # 5. ì™„ì „ ì‚­ì œ ì‹¤í–‰
    deletion_result = await execute_complete_erasure(request, approval_log)
    
    return {
        'status': 'completed',
        'deletion_id': deletion_result['deletion_id'],
        'items_deleted': deletion_result['items_deleted'],
        'completed_at': datetime.utcnow().isoformat()
    }

@dataclass
class AdminEraseRequest:
    """ê´€ë¦¬ì ì‚­ì œ ìš”ì²­"""
    user_id: Optional[str] = None
    document_ids: Optional[List[str]] = None
    deletion_type: str = "user_request"  # user_request, legal_order, gdpr_request
    reason: str = ""
    legal_basis: Optional[str] = None
    confirm_permanent: bool = False
    
    def __post_init__(self):
        if not self.confirm_permanent:
            raise ValueError("Permanent deletion must be explicitly confirmed")

async def execute_complete_erasure(request: AdminEraseRequest, approval_log: dict):
    """ì™„ì „ ì‚­ì œ ì‹¤í–‰"""
    
    deletion_id = f"admin-erase-{uuid4().hex[:16]}"
    items_deleted = 0
    
    try:
        if request.user_id:
            # ì‚¬ìš©ìì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ
            user_data = await get_all_user_data(request.user_id)
            
            for document in user_data.documents:
                await cascade_delete_manager.delete_document_cascade(
                    document.id, request.user_id
                )
                items_deleted += 1
            
            # ì‚¬ìš©ì ë©”íƒ€ë°ì´í„° ì‚­ì œ (GDPR ì¤€ìˆ˜)
            await anonymize_user_records(request.user_id)
            
        elif request.document_ids:
            # ì§€ì •ëœ ë¬¸ì„œë“¤ë§Œ ì‚­ì œ
            for doc_id in request.document_ids:
                await cascade_delete_manager.delete_document_cascade(
                    doc_id, "admin_delete"
                )
                items_deleted += 1
        
        # ì˜êµ¬ ì‚­ì œ í™•ì¸ ë¡œê·¸ (ê°ì‚¬ ëª©ì , ì‚­ì œë˜ì§€ ì•ŠìŒ)
        await log_permanent_deletion({
            'deletion_id': deletion_id,
            'admin_action': approval_log,
            'items_deleted': items_deleted,
            'deletion_verified': True,
            'retention_period': 'permanent_audit_log'  # ê°ì‚¬ ë¡œê·¸ëŠ” ì˜êµ¬ ë³´ê´€
        })
        
        return {
            'deletion_id': deletion_id,
            'items_deleted': items_deleted,
            'status': 'completed'
        }
        
    except Exception as e:
        await log_deletion_failure(deletion_id, str(e), approval_log)
        raise HTTPException(500, f"Deletion failed: {e}")
```

### ìë™ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬

```python
# ì¼ì¼ ì‹¤í–‰ë˜ëŠ” ë°ì´í„° ì •ë¦¬ ì‘ì—…
@scheduler.scheduled_job("cron", hour=2, minute=0)  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ ì‹¤í–‰
async def daily_data_retention_cleanup():
    """ì¼ì¼ ë°ì´í„° ë³´ê´€ ì •ì±… ì‹¤í–‰"""
    
    retention_manager = DataRetentionManager()
    
    try:
        # ë³´ê´€ ì •ì±… ì‹¤í–‰
        await retention_manager.execute_retention_policy()
        
        # ì •ë¦¬ í†µê³„ ìˆ˜ì§‘
        cleanup_stats = await retention_manager.get_cleanup_statistics()
        
        # ê´€ë¦¬ìì—ê²Œ ë³´ê³ 
        await send_admin_report("daily_cleanup", cleanup_stats)
        
    except Exception as e:
        logger.error(f"Daily cleanup failed: {e}")
        await send_admin_alert("cleanup_failure", str(e))

# ì‚¬ìš©ì ì•Œë¦¼ (ì‚­ì œ 7ì¼ ì „)
@scheduler.scheduled_job("cron", hour=10, minute=0)  # ë§¤ì¼ ì˜¤ì „ 10ì‹œ ì‹¤í–‰
async def notify_users_before_deletion():
    """ì‚­ì œ ì˜ˆì • ì‚¬ìš©ìë“¤ì—ê²Œ ì•Œë¦¼"""
    
    seven_days_later = datetime.utcnow() + timedelta(days=7)
    cutoff_date = seven_days_later - timedelta(days=180)
    
    users_to_notify = await get_users_with_expiring_data(cutoff_date)
    
    for user in users_to_notify:
        await send_deletion_notification(user)
```

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "embed_version \uad00\ub9ac \ubc0f Reindex All \uae30\ub2a5 \uc644\uc131", "status": "completed", "activeForm": "Completing embed_version management and Reindex All functionality"}, {"content": "\ubb38\uc11c \uc0ad\uc81c \uc2dc \ubca1\ud130 \uc2a4\ud1a0\uc5b4 \uc5f0\ucabd \uc0ad\uc81c \ubcf4\uc7a5", "status": "completed", "activeForm": "Ensuring vector store cascade deletion when documents are deleted"}, {"content": "180\uc77c \ubcf4\uad00/\uc0ad\uc81c \uc815\ucc45 \ubc0f /analytics/erase \uad00\ub9ac\uc790 \uacbd\ub85c", "status": "completed", "activeForm": "180-day retention/deletion policy and /analytics/erase admin endpoint"}, {"content": "Runbook 4\uc885 \uc791\uc131", "status": "in_progress", "activeForm": "Writing 4 types of runbooks"}, {"content": "\ub864\ubc31 \uacc4\ud68d \uc218\ub9bd", "status": "pending", "activeForm": "Establishing rollback plan"}, {"content": "\ucd5c\uc885 \ubb38\uc11c \uc5c5\ub370\uc774\ud2b8", "status": "pending", "activeForm": "Final documentation updates"}, {"content": "\ub370\uc774\ud130 \uac70\ubc84\ub108\uc2a4\uc640 \uc6b4\uc601 \ubb38\uc11c \ucd5c\uc885 \ubcf4\uace0\uc11c \uc791\uc131", "status": "pending", "activeForm": "Writing final data governance and operations documentation report"}]